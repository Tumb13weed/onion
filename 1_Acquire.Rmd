---
title: "Acquire the Onion Data"
author: "Amit Kapoor"
date: "1 July 2015"
output: html_document
---

## Data Sources

There are three place to get onion price and quantity information by market. 

1. [Agmarket](http://agmarknet.nic.in/) - This is the website run by the Directorate of Marketing & Inspection (DMI), Ministry of Agriculture, Government of India and provides daily price and arrival data for all agricultural commodities at national and state level. Unfortunately, the link to get Market-wise Daily Report for Specific Commodity (Onion for us) leads to a multipage aspx entry form to get data for each date. So it is like to require an involved scraper to get the data. Too much effort - Move on. Here is the best link to go to get what is available - http://agmarknet.nic.in/agnew/NationalBEnglish/SpecificCommodityWeeklyReport.aspx?ss=1

2. [Data.gov.in](https://data.gov.in/) - This is normally a good place to get government data in a machine readable form like csv or xml. The Variety-wise Daily Market Prices Data of Onion is available for each year as an XML but unfortunately it does not include quantity information that is needed. It would be good to have both price and quantity - so even though this is easy, lets see if we can get both from a different source. Here is the best link to go to get what is available - https://data.gov.in/catalog/variety-wise-daily-market-prices-data-onion#web_catalog_tabs_block_10

3. [NDRDF](http://nhrdf.org/en-us/) - This is the website of National Horticultural Research & Development Foundation and maintains a database on Market Arrivals and Price, Area and Production and Export Data for three commodities - Garlic, Onion and Potatoes. We are in luck! It also has data from 1996 onwards and has only got one form to fill to get the data in a tabular form. Lets use this. Here is the best link to got to get all that is available - http://nhrdf.org/en-us/DatabaseReports


## Scraping the Data

We need to scrape data from a table but we also need to submit a form to get the table. I will use a new library called rvest to do this. rvest is inspired from beautiful soup in python which I llke, so lets give it a go. Here is the link to rvest if you want to read more - http://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/

We will start by getting the [Dailywise Market Arrivals](http://nhrdf.org/en-us/DailyWiseMarketArrivals). The form on this page looks simple. But viewing source in the browser shows there form to fill with hidden fields and we will need to access it as a browser to get the session fields and then submit the form. First lets get the form.

```{r}
library(rvest)
library(magrittr)

url <- "http://nhrdf.org/en-us/DailyWiseMarketArrivals"
pg <- html(url)

# Set a session - then get the form - extract the first one
pg.session <- html_session(url)
pg.form <- html_form(pg.session)[[1]]

```

Now that we have the form, let see if we can fill the form. Even though the form gives us options to choose by name, inspecting the html shows clearly that the we need to add number for each one of the fields. Leaving them blank (for month, year and market) makes it equal to all. Lets get our data. (For testing. don't leave all them blank)

```{r}
# Set scraping value 
# Crop = 1 for Onion, Year = numeric (blank for all years)
# MonthName = 1 for Jan and so on (blank for all months)
# Market = blank for all markets
crop <- 1
month <- 1
year <- 2015
market <- ""

# Fill the form with the values
pg.form.filled <- set_values(pg.form,
                      'dnn$dnnLANG$selectCulture' = "en-US",
                      'dnn$ctr966$DailyWiseMarketArrivals$Crop' = crop,
                      'dnn$ctr966$DailyWiseMarketArrivals$Year'= year,
                      'dnn$ctr966$DailyWiseMarketArrivals$Market' = market,
                      'dnn$ctr966$DailyWiseMarketArrivals$MonthName' = month)

# Submit the form and get the page
pg.submit <- submit_form(pg.session, pg.form.filled,
                         submit = 'dnn$ctr966$DailyWiseMarketArrivals$btnSearch')
pg.out <- html(pg.submit)

```

Now that we have the html with our table, we need to find it on our page using the css selector. Then convert it into a data frame. And then write it to a csv file to store for the next step. 

```{r}
# Read the page and convert to data frame
pg.table <-  pg.out %>% 
            html_node("#dnn_ctr966_DailyWiseMarketArrivals_GridView1")  %>%
            html_table()

str(pg.table)

# 
file <- paste("data/onion", as.character(month), as.character(year), ".csv", sep="")
write.csv(pg.table, file = file, quote = FALSE, row.names = FALSE)
```

Ready to go forward.




